import os
import numpy as np
import argparse
from ultralytics import YOLO
import matplotlib.pyplot as plt

def get_model_info(model):
    """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –º–æ–¥–µ–ª—å"""
    try:
        total_params = sum(p.numel() for p in model.model.parameters())
        trainable_params = sum(p.numel() for p in model.model.parameters() if p.requires_grad)
        model_size_mb = total_params * 4 / 1024 / 1024
        info = {
            'total_params': total_params,
            'trainable_params': trainable_params,
            'model_size_mb': model_size_mb,
            'model_type': type(model.model).__name__
        }
        return info
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–Ω—è —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –º–æ–¥–µ–ª—å: {e}")
        return {}
    
def create_average_plots(all_results, save_dir, class_names=None):
    """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—ñ—Ö –≥—Ä–∞—Ñ—ñ–∫—ñ–≤ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –≤—Å—ñ—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤"""
    os.makedirs(save_dir, exist_ok=True)
    
    # –ó–±–∏—Ä–∞—î–º–æ –≤—Å—ñ –º–µ—Ç—Ä–∏–∫–∏
    map50_values = [r.box.map50 for r in all_results]
    map_values = [r.box.map for r in all_results]
    precision_values = [r.box.mp for r in all_results]
    recall_values = [r.box.mr for r in all_results]
    
    # –°–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è
    avg_map50 = np.mean(map50_values)
    avg_map = np.mean(map_values)
    avg_precision = np.mean(precision_values)
    avg_recall = np.mean(recall_values)
    
    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ñ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è
    std_map50 = np.std(map50_values)
    std_map = np.std(map_values)
    std_precision = np.std(precision_values)
    std_recall = np.std(recall_values)
    
    # 1. –°–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –∑ –¥–æ–≤—ñ—Ä—á–∏–º–∏ —ñ–Ω—Ç–µ—Ä–≤–∞–ª–∞–º–∏
    plt.figure(figsize=(10, 6))
    
    metrics = ['mAP@0.5', 'mAP@0.5:0.95', 'Precision', 'Recall']
    means = [avg_map50, avg_map, avg_precision, avg_recall]
    stds = [std_map50, std_map, std_precision, std_recall]
    
    x_pos = np.arange(len(metrics))
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']
    
    bars = plt.bar(x_pos, means, yerr=stds, capsize=5, color=colors, alpha=0.7, 
                   edgecolor='black', linewidth=1.2)
    
    plt.xlabel('–ú–µ—Ç—Ä–∏–∫–∏')
    plt.ylabel('–ó–Ω–∞—á–µ–Ω–Ω—è')
    plt.title('–°–µ—Ä–µ–¥–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–º–∏ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è–º–∏')
    plt.xticks(x_pos, metrics, rotation=45)
    plt.grid(True, alpha=0.3, axis='y')
    
    # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ–≤–ø—Ü—ñ
    for i, (bar, mean, std) in enumerate(zip(bars, means, stds)):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.01, 
                f'{mean:.3f}¬±{std:.3f}', ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'average_metrics.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    # 2. –ì—ñ—Å—Ç–æ–≥—Ä–∞–º–∏ —Ä–æ–∑–ø–æ–¥—ñ–ª—É –º–µ—Ç—Ä–∏–∫
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    plt.hist(map50_values, bins=max(3, len(map50_values)//2), alpha=0.7, color='blue', edgecolor='black')
    plt.axvline(avg_map50, color='red', linestyle='--', linewidth=2, label=f'–°–µ—Ä–µ–¥–Ω—î: {avg_map50:.4f}')
    plt.title('–†–æ–∑–ø–æ–¥—ñ–ª mAP@0.5')
    plt.xlabel('mAP@0.5')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 2)
    plt.hist(map_values, bins=max(3, len(map_values)//2), alpha=0.7, color='green', edgecolor='black')
    plt.axvline(avg_map, color='red', linestyle='--', linewidth=2, label=f'–°–µ—Ä–µ–¥–Ω—î: {avg_map:.4f}')
    plt.title('–†–æ–∑–ø–æ–¥—ñ–ª mAP@0.5:0.95')
    plt.xlabel('mAP@0.5:0.95')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 3)
    plt.hist(precision_values, bins=max(3, len(precision_values)//2), alpha=0.7, color='red', edgecolor='black')
    plt.axvline(avg_precision, color='blue', linestyle='--', linewidth=2, label=f'–°–µ—Ä–µ–¥–Ω—î: {avg_precision:.4f}')
    plt.title('–†–æ–∑–ø–æ–¥—ñ–ª Precision')
    plt.xlabel('Precision')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.subplot(2, 2, 4)
    plt.hist(recall_values, bins=max(3, len(recall_values)//2), alpha=0.7, color='magenta', edgecolor='black')
    plt.axvline(avg_recall, color='blue', linestyle='--', linewidth=2, label=f'–°–µ—Ä–µ–¥–Ω—î: {avg_recall:.4f}')
    plt.title('–†–æ–∑–ø–æ–¥—ñ–ª Recall')
    plt.xlabel('Recall')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'metrics_distribution.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    return {
        'avg_map50': avg_map50,
        'avg_map': avg_map,
        'avg_precision': avg_precision,
        'avg_recall': avg_recall,
        'std_map50': std_map50,
        'std_map': std_map,
        'std_precision': std_precision,
        'std_recall': std_recall
    }

def evaluate_yolo_model(model_path, data_yaml_path, split='test', project='YOLO_evaluation', 
                        name='evaluation', conf=0.25, iou=0.45, device='0', num_evaluations=10):
    """
    –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ YOLO –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—ñ –∑ –≤–∏–≤–æ–¥–æ–º –¥–µ—Ç–∞–ª—å–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫, –≤–∏–∫–æ–Ω–∞–Ω–æ—é –∫—ñ–ª—å–∫–∞ —Ä–∞–∑—ñ–≤
    """
    print("\n--- –ü–æ—á–∞—Ç–æ–∫ –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ ---")
    print(f"–ú–æ–¥–µ–ª—å: {model_path}")
    print(f"–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –¥–∞—Ç–∞—Å–µ—Ç—É: {data_yaml_path}")
    print(f"–†–æ–∑–¥—ñ–ª –¥–ª—è –æ—Ü—ñ–Ω–∫–∏: {split}")
    print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ—Ü—ñ–Ω–æ–∫: {num_evaluations}")
    
    if not os.path.exists(model_path):
        print(f"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –º–æ–¥–µ–ª—ñ {model_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return None
    
    if not os.path.exists(data_yaml_path):
        print(f"–ü–æ–º–∏–ª–∫–∞: –§–∞–π–ª –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –¥–∞—Ç–∞—Å–µ—Ç—É {data_yaml_path} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return None
    
    try:
        print("–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
        model = YOLO(model_path)
        
        model_info = get_model_info(model)
        if model_info:
            print("\n--- –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –º–æ–¥–µ–ª—å ---")
            print(f"–¢–∏–ø –º–æ–¥–µ–ª—ñ: {model_info.get('model_type', '–ù–µ–≤—ñ–¥–æ–º–æ')}")
            print(f"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤: {model_info.get('total_params', 0):,}")
            print(f"–ù–∞–≤—á–∞–ª—å–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏: {model_info.get('trainable_params', 0):,}")
            print(f"–†–æ–∑–º—ñ—Ä –º–æ–¥–µ–ª—ñ: {model_info.get('model_size_mb', 0):.1f} MB")
        
        all_results = []
        total_confusion_matrix = None
        class_names = None
        
        for i in range(num_evaluations):
            print(f"\n–í–∏–∫–æ–Ω–∞–Ω–Ω—è –æ—Ü—ñ–Ω–∫–∏ {i+1} –∑ {num_evaluations}...")
            results = model.val(
                data=data_yaml_path,
                split=split,
                conf=conf,
                iou=iou,
                device=device,
                save=True,
                save_json=True,
                project=project,
                name=f"{name}_{i+1}",
                plots=True,
                verbose=True,
                exist_ok=True
            )
            
            all_results.append(results)
            print(f"   mAP@0.5: {results.box.map50:.4f}, mAP@0.5:0.95: {results.box.map:.4f}")
            
            # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Ç–∞ –Ω–∞–∫–æ–ø–∏—á–µ–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—ñ –ø–æ–º–∏–ª–æ–∫
            if hasattr(results, 'confusion_matrix') and results.confusion_matrix is not None:
                if total_confusion_matrix is None:
                    class_names = results.names
                    total_confusion_matrix = np.zeros_like(results.confusion_matrix.matrix)
                total_confusion_matrix += results.confusion_matrix.matrix
            elif total_confusion_matrix is None:
                 print("\n–£–≤–∞–≥–∞: –ú–∞—Ç—Ä–∏—Ü—è –ø–æ–º–∏–ª–æ–∫ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö. –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ —Ä–æ–∑—à–∏—Ä–µ–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫ –±—É–¥–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ.")

        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—é –¥–ª—è –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤
        save_dir = os.path.join(project, f"{name}_averaged_results")
        os.makedirs(save_dir, exist_ok=True)
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Ä–µ–¥–Ω—ñ –≥—Ä–∞—Ñ—ñ–∫–∏ —Ç–∞ –æ—Ç—Ä–∏–º—É—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print("\n–°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Å–µ—Ä–µ–¥–Ω—ñ—Ö –≥—Ä–∞—Ñ—ñ–∫—ñ–≤...")
        stats = create_average_plots(all_results, save_dir)
        
        print("\n" + "="*60)
        print("–§–Ü–ù–ê–õ–¨–ù–Ü –†–ï–ó–£–õ–¨–¢–ê–¢–ò –û–¶–Ü–ù–ö–ò –ú–û–î–ï–õ–Ü")
        print("="*60)
        print(f"\nüìä –°–µ—Ä–µ–¥–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ ({num_evaluations} —ñ—Ç–µ—Ä–∞—Ü—ñ–π):")
        print(f"   ‚Ä¢ mAP@0.5:      {stats['avg_map50']:.4f} ¬± {stats['std_map50']:.4f}")
        print(f"   ‚Ä¢ mAP@0.5:0.95: {stats['avg_map']:.4f} ¬± {stats['std_map']:.4f}")
        print(f"   ‚Ä¢ Precision:    {stats['avg_precision']:.4f} ¬± {stats['std_precision']:.4f}")
        print(f"   ‚Ä¢ Recall:       {stats['avg_recall']:.4f} ¬± {stats['std_recall']:.4f}")
        
        print("\nüìà –í–∞—Ä—ñ–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å –º–µ—Ç—Ä–∏–∫:")
        print(f"   ‚Ä¢ mAP@0.5 CV:      {(stats['std_map50']/stats['avg_map50']*100):.2f}%")
        print(f"   ‚Ä¢ mAP@0.5:0.95 CV: {(stats['std_map']/stats['avg_map']*100):.2f}%")
        print(f"   ‚Ä¢ Precision CV:    {(stats['std_precision']/stats['avg_precision']*100):.2f}%")
        print(f"   ‚Ä¢ Recall CV:       {(stats['std_recall']/stats['avg_recall']*100):.2f}%")
        
        # --- –ù–û–í–ò–ô –ë–õ–û–ö: –†–û–ó–†–ê–•–£–ù–û–ö TP, FP, FN, TN –¢–ê ACCURACY ---
        if total_confusion_matrix is not None and class_names is not None:
            print("\n" + "="*60)
            print(f"–†–û–ó–®–ò–†–ï–ù–Ü –ú–ï–¢–†–ò–ö–ò –ü–û –ö–õ–ê–°–ê–• (–Ω–∞ –æ—Å–Ω–æ–≤—ñ {num_evaluations} –∑–∞–ø—É—Å–∫—ñ–≤)")
            print("="*60)

            all_class_metrics = {}
            num_classes = len(class_names)

            for i in range(num_classes):
                class_name = class_names[i]
                
                tp = total_confusion_matrix[i, i]
                fp = total_confusion_matrix[:, i].sum() - tp
                fn = total_confusion_matrix[i, :].sum() - tp
                tn = total_confusion_matrix.sum() - (tp + fp + fn)

                denominator = tp + tn + fp + fn
                accuracy = (tp + tn) / denominator if denominator > 0 else 0
                
                all_class_metrics[class_name] = {
                    'TP': int(tp), 'FP': int(fp), 'FN': int(fn), 'TN': int(tn),
                    'Accuracy': f"{accuracy:.4f}"
                }

                print(f"\n‚úÖ –ö–ª–∞—Å: '{class_name}'")
                print(f"   - True Positives (TP):    {int(tp)}")
                print(f"   - False Positives (FP):   {int(fp)}")
                print(f"   - False Negatives (FN):   {int(fn)}")
                print(f"   - True Negatives (TN):    {int(tn)}")
                print(f"   - Accuracy:               {accuracy:.4f}")

            # –ó–∞–≥–∞–ª—å–Ω–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å (Overall Accuracy)
            total_tp = np.diag(total_confusion_matrix).sum()
            total_elements = total_confusion_matrix.sum()
            overall_accuracy = total_tp / total_elements if total_elements > 0 else 0

            print("\n" + "-"*60)
            print("üéØ –ó–ê–ì–ê–õ–¨–ù–ê –¢–û–ß–ù–Ü–°–¢–¨ (OVERALL ACCURACY)")
            print("   –†–æ–∑—Ä–∞—Ö–æ–≤–∞–Ω–∞ —è–∫: sum(TP) / sum(–≤—Å—ñ—Ö –µ–ª–µ–º–µ–Ω—Ç—ñ–≤ –º–∞—Ç—Ä–∏—Ü—ñ)")
            print(f"   Accuracy = {total_tp:.0f} / {total_elements:.0f} = {overall_accuracy:.4f}")
            print("-"*60)
            print("\n*–ü—Ä–∏–º—ñ—Ç–∫–∞: –í –∑–∞–¥–∞—á–∞—Ö –¥–µ—Ç–µ–∫—Ü—ñ—ó –æ–±'—î–∫—Ç—ñ–≤, 'TN' —Ç–∞ 'Accuracy' —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç—É—é—Ç—å—Å—è")
            print("—ñ–Ω–∞–∫—à–µ, –Ω—ñ–∂ —É –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó. –¢—É—Ç TN ‚Äì —Ü–µ –æ–±'—î–∫—Ç–∏ —ñ–Ω—à–∏—Ö –∫–ª–∞—Å—ñ–≤, —â–æ –Ω–µ –±—É–ª–∏")
            print("–ø–æ–º–∏–ª–∫–æ–≤–æ –≤—ñ–¥–Ω–µ—Å–µ–Ω—ñ –¥–æ –ø–æ—Ç–æ—á–Ω–æ–≥–æ –∫–ª–∞—Å—É.")

            stats['per_class_metrics'] = all_class_metrics
            stats['overall_accuracy'] = overall_accuracy
        
        print(f"\nüíæ –£—Å—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ –≤: {save_dir}")
        
        return stats
        
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ: {str(e)}")
        return None

def parse_arguments():
    parser = argparse.ArgumentParser(description='–û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ YOLOv8')
    parser.add_argument('--model', type=str, required=True, help='–®–ª—è—Ö –¥–æ –º–æ–¥–µ–ª—ñ YOLOv8 (.pt)')
    parser.add_argument('--data', type=str, required=True, help='–®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É data.yaml')
    parser.add_argument('--split', type=str, default='test', choices=['train', 'val', 'test'], help='–†–æ–∑–¥—ñ–ª –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –æ—Ü—ñ–Ω–∫–∏')
    parser.add_argument('--conf', type=float, default=0.25, help='–ü–æ—Ä—ñ–≥ –≤–ø–µ–≤–Ω–µ–Ω–æ—Å—Ç—ñ')
    parser.add_argument('--iou', type=float, default=0.45, help='–ü–æ—Ä—ñ–≥ IoU –¥–ª—è NMS')
    parser.add_argument('--device', type=str, default='0', help='–ü—Ä–∏—Å—Ç—Ä—ñ–π –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É (CPU: "cpu", GPU: 0,1,2...)')
    parser.add_argument('--num_evaluations', type=int, default=1, help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ—Ü—ñ–Ω–æ–∫')
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    evaluate_yolo_model(
        model_path=args.model,
        data_yaml_path=args.data,
        split=args.split,
        project="YOLO_test_IMG",
        name="test",
        conf=args.conf,
        iou=args.iou,
        device=args.device,
        num_evaluations=args.num_evaluations
    )